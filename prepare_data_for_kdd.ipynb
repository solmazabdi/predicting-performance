{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps need to be done for preprocessing:** <br>\n",
    "1: Remove the students whose activities are below a threshold<br>\n",
    "2: Divide the dataset into train and test set<br>\n",
    "3: Create a Q-matrix<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def prvar(__x):\n",
    "    print(traceback.extract_stack(limit=2)[0][3][6:][:-1],\"=\",__x)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_kddcup10(folder_name, course_name, train_file, test_file, kc_col_name, min_interactions_per_user, remove_nan_skills, verbose,\\\n",
    "                    drop_duplicates=True):\n",
    "    '''\n",
    "    Reading input files\n",
    "    Drop rows for which topic is not determined (it does not happen for RiPPLE files)\n",
    "    Return the pre-processed file and Q-matrix.\n",
    "    \n",
    "    Arguments:\n",
    "    folder_name -- path to the folder containig kdd files (algebra05, bridge_algebra06)\n",
    "    course_name -- name of the course for which pre_processing is executed\n",
    "    train_file -- original train_file provided by KDD cup organizers\n",
    "    test_file -- original test_file provided by KDD cup organizers\n",
    "    kc_col_name -- Skills id column\n",
    "    min_interactions_per_user -- minimum number of interactions per student\n",
    "    drop_duplicates -- if True, drop duplicates from dataset\n",
    "    \n",
    "    Outputs:\n",
    "    data -- preprocessed dataset (pandas DataFrame)\n",
    "    Q_mat -- corresponding q-matrix (item-skill relationships sparse array)\n",
    "    '''\n",
    "    if not os.path.exists(folder_name):\n",
    "        print(\"The provided path for the data is invalid and the function will not be executed.\")\n",
    "        return None, None, None, None\n",
    "        \n",
    "    # reading csv file containing information about students' practice (attempt) history\n",
    "    # from the train and test file provided by KDD organizer and then concatante them.\n",
    "    train_file_path = folder_name  +'/'+  course_name  +'/'+ train_file\n",
    "    df_train = pd.read_csv(train_file_path, delimiter='\\t').rename(columns={\n",
    "        'Anon Student Id': 'user_id',\n",
    "        'Problem Name': 'pb_id',\n",
    "        'Step Name': 'step_id',\n",
    "        kc_col_name: 'kc_id',\n",
    "        'First Transaction Time': 'timestamp',\n",
    "        'Correct First Attempt': 'correct'\n",
    "    })[['user_id', 'pb_id', 'step_id' ,'correct', 'timestamp', 'kc_id']]\n",
    "    if verbose:\n",
    "        initial_shape = df_train.shape[0]\n",
    "        print(\"Opened KDD Cup 2010 data. Output: {} samples.\".format(initial_shape))\n",
    "    test_file_path = folder_name  +'/'+  course_name  +'/'+ test_file\n",
    "    df_test = pd.read_csv(test_file_path, delimiter='\\t').rename(columns={\n",
    "        'Anon Student Id': 'user_id',\n",
    "        'Problem Name': 'pb_id',\n",
    "        'Step Name': 'step_id',\n",
    "        kc_col_name: 'kc_id',\n",
    "        'First Transaction Time': 'timestamp',\n",
    "        'Correct First Attempt': 'correct'\n",
    "    })[['user_id', 'pb_id', 'step_id' ,'correct', 'timestamp', 'kc_id']]\n",
    "    if verbose:\n",
    "        initial_shape = df_test.shape[0]\n",
    "        print(\"Opened KDD Cup 2010 data. Output: {} samples.\".format(initial_shape))\n",
    "    df_train['group'] = 'train'\n",
    "    df_test['group'] = 'test'\n",
    "    frames = [df_train, df_test]\n",
    "    data = pd.concat(frames)\n",
    "    del df_train\n",
    "    del df_test\n",
    "    #removing rows with empty value for KC from our dataframe\n",
    "    if remove_nan_skills:\n",
    "        data = data[~data[\"kc_id\"].isnull()]\n",
    "        if verbose:\n",
    "            print(\"Removed {} samples with NaN skills.\".format(data.shape[0]-initial_shape))\n",
    "            initial_shape = data.shape[0]\n",
    "    else:\n",
    "        data.loc[data[\"kc_id\"].isnull(), \"kc_id\"] = 'NaN'\n",
    "    \n",
    "    data = data[data['correct'].isin([0,1])] # Remove potential continuous outcomes\n",
    "    if verbose:\n",
    "        print(\"Removed {} samples with non-binary outcomes.\".format(data.shape[0]-initial_shape))\n",
    "        initial_shape = data.shape[0]\n",
    "    data['correct'] = data['correct'].astype(np.int32) # Cast outcome as int32\n",
    "    \n",
    "    \n",
    "    data = data.groupby(\"user_id\").filter(lambda x: len(x) >= min_interactions_per_user)\n",
    "    if verbose:\n",
    "        print('Removed {} samples (users with less than {} interactions).'.format((data.shape[0]-initial_shape,\n",
    "                                                         min_interactions_per_user)))\n",
    "        initial_shape = data.shape[0]\n",
    "\n",
    "    # Create variables\n",
    "    data[\"item_id\"] = data[\"pb_id\"]+\":\"+data[\"step_id\"]\n",
    "    data = data[['user_id', 'item_id', 'kc_id', 'correct', 'timestamp', 'group']]\n",
    "        \n",
    "        \n",
    "    # Transform ids into numeric\n",
    "    data[\"item_id\"] = np.unique(data[\"item_id\"], return_inverse=True)[1]\n",
    "    data[\"user_id\"] = np.unique(data[\"user_id\"], return_inverse=True)[1]\n",
    "\n",
    "    # Create list of KCs\n",
    "    listOfKC = []\n",
    "    for kc_raw in data[\"kc_id\"].unique():\n",
    "        for elt in kc_raw.split('~~'):\n",
    "            listOfKC.append(elt)\n",
    "    listOfKC = np.unique(listOfKC)\n",
    "\n",
    "    dict1_kc = {}\n",
    "    dict2_kc = {}\n",
    "    for k, v in enumerate(listOfKC):\n",
    "        dict1_kc[v] = k\n",
    "        dict2_kc[k] = v\n",
    "\n",
    "    #df.reset_index(inplace=True, drop=True) # Add unique identifier of the row\n",
    "    #df[\"inter_id\"] = df.index\n",
    "\n",
    "    # Build Q-matrix\n",
    "    Q_mat = np.zeros((len(data[\"item_id\"].unique()), len(listOfKC)))\n",
    "    item_skill = np.array(data[[\"item_id\",\"kc_id\"]])\n",
    "    for i in range(len(item_skill)):\n",
    "        splitted_kc = item_skill[i,1].split('~~')\n",
    "        for kc in splitted_kc:\n",
    "            Q_mat[item_skill[i,0],dict1_kc[kc]] = 1\n",
    "    if verbose:\n",
    "        print(\"Computed q-matrix. Shape: {}.\".format(Q_mat.shape))\n",
    "\n",
    "    data = data[['user_id', 'item_id', 'timestamp', 'correct', 'kc_id', 'group']]\n",
    "\n",
    "    data['timestamp'] =  pd.to_datetime(data['timestamp'])#, dayfirst=True)\n",
    "    data.sort_values(by=\"timestamp\", inplace=True) #first, timestamp should be converted to datetime\n",
    "    data.reset_index(inplace=True, drop=True)    \n",
    "        \n",
    "    # Remove potential duplicates\n",
    "    data.drop_duplicates(subset= ['user_id', 'item_id', 'timestamp', 'correct', 'kc_id'], inplace=True)\n",
    "    data.reset_index(inplace=True, drop=True)   \n",
    "    if verbose:\n",
    "        print(\"Removed {} duplicated samples.\".format(data.shape[0] - initial_shape))\n",
    "        initial_shape = data.shape[0]\n",
    "        \n",
    "    train_set = data[data['group'] == 'train']\n",
    "    train_set.reset_index(inplace=True, drop=True)\n",
    "    train_set['timestamp'] =  pd.to_datetime(train_set['timestamp'])#, dayfirst=True)\n",
    "    train_set.sort_values(by=\"timestamp\", inplace=True) #first, timestamp should be converted to datetime\n",
    "    train_set.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    test_set = data[data['group'] == 'test']\n",
    "    test_set.reset_index(inplace=True, drop=True)\n",
    "    test_set['timestamp'] =  pd.to_datetime(test_set['timestamp'])#, dayfirst=True)\n",
    "    test_set.sort_values(by=\"timestamp\", inplace=True) #first, timestamp should be converted to datetime\n",
    "    test_set.reset_index(inplace=True, drop=True)\n",
    "    print(\"Data preprocessing done. Final output: {} samples.\".format((data.shape[0])))\n",
    "\n",
    "    # Save data\n",
    "    if not os.path.isdir(folder_name+'/'+ course_name+\"/processed\"):\n",
    "        os.makedirs(folder_name+'/'+ course_name+\"/processed\")\n",
    "    sparse.save_npz(folder_name+'/'+ course_name+\"/processed/q_mat.npz\", sparse.csr_matrix(Q_mat))\n",
    "    data.to_csv(folder_name+'/'+ course_name+\"/processed/preprocessed_data.csv\", index=False)\n",
    "    listOfKC = list(listOfKC)\n",
    "    # Save train-test data\n",
    "    train_set.to_csv(folder_name+'/'+ course_name+\"/processed/train_set.csv\", encoding='utf-8', index = False)\n",
    "    test_set.to_csv(folder_name+'/'+ course_name+\"/processed/test_set.csv\", encoding='utf-8', index = False)\n",
    "    \n",
    "    with open(folder_name+'/'+ course_name+'/processed/dict_of_kc.json', 'w') as fp:\n",
    "        json.dump(dict1_kc, fp)\n",
    "\n",
    "    \n",
    "    return data, Q_mat, listOfKC, dict1_kc, train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_processed_data, q_mat, listOfKC, dict_of_kc, train_set, test_set = prepare_kddcup10('data/kdd', 'bridge_algebra06', \\\n",
    "#                                                                    'bridge_to_algebra_2006_2007_train.txt', \\\n",
    "#                                                                    'bridge_to_algebra_2006_2007_master.txt',\\\n",
    "#                                                                    'KC(SubSkills)', 5, True, False, True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
