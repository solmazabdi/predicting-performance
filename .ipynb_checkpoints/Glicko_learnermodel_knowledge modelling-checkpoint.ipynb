{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glicko-based learner model\n",
    "This is an implementation of Glicko rating system. This mdoel estimates students' knowledge state based on their performance on attempting learning resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "import prepare_data_for_kdd as kdd_pdr\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations and General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A constant which is used to standardize the logistic function\n",
    "Q = math.log(10)/400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utctime():\n",
    "    \"\"\"A function like :func:`time.time` but it uses a time of UTC.\"\"\"\n",
    "    return time.mktime(datetime.datetime.utcnow().timetuple())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volatilize(t, sigma): # rememeber to de-comment the atual row for updating sigma. \n",
    "    '''\n",
    "    This only applies to students and does not apply to items. \n",
    "    Becasue, for items, we don't expect their difficulty to be \n",
    "    fluctuated with the passage of time. \n",
    "    Arguments:\n",
    "    t -- time difference (in days)\n",
    "    sigma -- previous rating deviation for the student on the topic\n",
    "    \n",
    "    Output:\n",
    "    sigma_o -- updated rating deviation for the student on the topic as the result of passing time.\n",
    "    '''\n",
    "    c = 50\n",
    "#     print(\"time difference is:\", t)\n",
    "#     print(\"pre sigma is:\", sigma)\n",
    "    sigma_o = min(math.sqrt(sigma ** 2 + c ** 2 * t), 350)\n",
    "    return sigma_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_difference(prev_timestamp, curr_timestamp):\n",
    "    '''\n",
    "    Caculates the time difference between the current attempt on a topic\n",
    "    and the previous attempt on the same topic.\n",
    "    '''\n",
    "    prev_timestamp = pd._libs.tslib.Timestamp(prev_timestamp)\n",
    "    curr_timestamp = pd._libs.tslib.Timestamp(curr_timestamp)\n",
    "    td = float((curr_timestamp - prev_timestamp).total_seconds())/86400\n",
    "    return td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_time_update(uid, qid, c_sigma, timestamp):\n",
    "    '''\n",
    "    This function gets the time difference between the timestamp of the current interactions, and\n",
    "    the last time that each topic associated with the question are attempted by the student. \n",
    "    If it is the first time that a student attempts a topic (it's corresponding value in the attempted_at\n",
    "    dataframe is None), then nothing would happen. Otherwise, the time difference is calculated and \n",
    "    rating deviation of students are updated accordingly. \n",
    "    \n",
    "    Arguments: \n",
    "    uid -- student id\n",
    "    qid -- question id \n",
    "    c_sigma -- array of current Rating Deviation for the student on all topics\n",
    "    timestamp -- Timestamp associated with the current interaction\n",
    "    \n",
    "    Output:\n",
    "    c_sigma -- updated Rating Deviation for the student on all topics as the result of passage of time\n",
    "    '''\n",
    "    for m in range(tSize):\n",
    "        if q_mat[qid,m] != 0: #check this \n",
    "            if not pd.isnull(attempted_at.iloc[uid][m]):\n",
    "                # calculate the time difference as the multiplication of a day (24h)\n",
    "                td = get_time_difference(attempted_at.iloc[uid][m], timestamp)\n",
    "                # Update the timedate of the last attempt in the attempted_at dataframe by the current timestamp.\n",
    "                attempted_at.iloc[uid][m] = pd._libs.tslib.Timestamp(timestamp)\n",
    "                # call volatilize function to update rating deviation as the passage of time if it was bigger than one hour.\n",
    "                if td >= 0.04167:\n",
    "                    c_sigma[m] = volatilize(td, c_sigma[m])\n",
    "            else:\n",
    "                attempted_at.iloc[uid][m] = pd._libs.tslib.Timestamp(timestamp)   \n",
    "    return c_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_impact(sigma):\n",
    "    \"\"\"The original form is `g(RD)`. This function reduces the impact of\n",
    "     interactions as a function of an opponent's RD.\n",
    "    \"\"\"\n",
    "    return (1 + float((3 * (Q ** 2) * sigma ** 2)) / float(math.pi ** 2)) ** -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expect_score(rating, other_rating, impact):\n",
    "    \"\"\"\n",
    "    Expected score of the interaction.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + 10 ** (impact * (rating - other_rating) / -400.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''calculating RMSE based on model predictions and actual responses'''\n",
    "def CalculateRMSE(Output, ground, I):\n",
    "    Output = np.array(Output)\n",
    "    ground = np.array(ground)\n",
    "    error = (Output - ground) \n",
    "#     print(\"error is: \", error)\n",
    "    err_sqr = error*error\n",
    "#     print(\"error is: \", err_sqr)\n",
    "#     print(I)\n",
    "    RMSE = math.sqrt(err_sqr.sum()/I)\n",
    "#     RMSE = math.sqrt(err_sqr.sum()/I.sum())\n",
    "    return RMSE  \n",
    "def auc_roc(y, pred): \n",
    "    y = np.array(y)\n",
    "#     print(\"y is: \", y)\n",
    "#     print(\"pred is: \", pred)\n",
    "    pred = np.array(pred)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr) \n",
    "    return auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glicko with upper and lower threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_topic_glicko_item_training_with_thresholds(df):\n",
    "    \"\"\"\n",
    "    Implementation of multi-variate_Glicko rating system for adaptive educational learning platforms.\n",
    "    This function is used on the training set to learn the difficulty of items. Once the difficulty of \n",
    "    items are learneres by this function, they are used in another training function to learn the \n",
    "    competency of students. \n",
    "    The difference between this function and the previous functions is that the value of update to the knowledge \n",
    "    state of the students follows a minimum and maximum threshold. \n",
    "    Arguments:\n",
    "    df -- train data in the form of Pandas data frame\n",
    "    \n",
    "    Output:\n",
    "    \"\"\"\n",
    "    ##### probably, we need to use transpose of q_mat(check)#####\n",
    "    print(\"multivariate Glicko execution for learning item difficulty is started.\") \n",
    "    # For students we create a dataframe in which \n",
    "    correct = 0 #whether the question was not correctly or not\n",
    "    prob_1 = [] # a list for storing the probability of a correct response to a question\n",
    "    prob_2 = [] # a list fpr expected outcome by both RDs\n",
    "    actual = [] # a list for storing the actual response to a question.\n",
    "    response = np.zeros((len(df), 1))\n",
    "    d_square_inv = 0\n",
    "    difference = 0\n",
    "    for count, (index, item) in enumerate(df.iterrows()):\n",
    "        df_session_user = pd.DataFrame()\n",
    "        # Step 0: Initialization\n",
    "        uid = item['user_id']\n",
    "        qid = item['item_id']\n",
    "        timestamp = item['timestamp']\n",
    "        correct = item['correct']\n",
    "        actual.append(correct) # keeping the actual outcome of each interaction.\n",
    "        d = question_difficulty[qid] #getting the difficulty of the question qid from difficulty matrix\n",
    "        d_sigma = question_sigma[qid] #getting the sigma for difficulty of the question qid from difficulty matrix\n",
    "        c = learner_competency[uid] * q_mat[qid] # getting the competency of the student on the topics associated with the question\n",
    "        c_avg = np.sum(c)/np.count_nonzero(c) # weighted average competency of the student on the topics associated with the question\n",
    "        c_sigma = learner_sigma[uid] * q_mat[qid] # getting the sigma for the student on the topics associated with the question\n",
    "        \n",
    "        # Step 1: Update the Rating deviation for the student as the result of passage of time.\n",
    "        # If it is the first time of attempting a topic, then, passage of time is zero and this function does nothing.\n",
    "        c_sigma = sigma_time_update(uid, qid, c_sigma, timestamp)\n",
    "        c_sigma_avg = np.sum(c_sigma)/np.count_nonzero(c_sigma)\n",
    "        \n",
    "        # Step 2: Calculate required coefficients\n",
    "        ## 2-1: Calculate g(RD) for items and students\n",
    "        # 2-1-a: g(RD) for item\n",
    "        q_impact = reduce_impact(d_sigma)\n",
    "        # 2-1-b: g(RD) for student on each topic\n",
    "        u_impact = np.zeros(tSize)\n",
    "\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                u_impact[m] = reduce_impact(c_sigma[m]) \n",
    "        u_impact_avg = np.sum(u_impact)/np.count_nonzero(u_impact) # or reduce_impact(c_sigma_avg) \n",
    "        \n",
    "        ## 2-2: calculate the expected result from student and item point of view\n",
    "        # 2-2-a: calculate the expected outcome based on proficiency on each topic\n",
    "        u_topic_expected_result = np.zeros(tSize)\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "#                 print(m)\n",
    "#                 print(\"c[m] is\", c[m])\n",
    "                u_topic_expected_result[m] = expect_score(c[m], d, q_impact)   \n",
    "        # 2-2-b: calculate the expected outcome based on proficiency on all topics\n",
    "        u_item_expected_result = expect_score(c_avg, d, q_impact)\n",
    "        prob_1.append(u_item_expected_result)\n",
    "        # 2-2-c: calculate the expected outcome from the item point of view\n",
    "        q_expected_result = expect_score(d, c_avg, u_impact_avg)\n",
    "        # 2-2-d: calculate the expected outcome of a question for a student based on both rating deviations\n",
    "        mix_sigma = math.sqrt(c_sigma_avg**2 + d_sigma **2)\n",
    "        mix_impact = reduce_impact(mix_sigma)\n",
    "        mix_expected_result = expect_score(c_avg, d, mix_impact)\n",
    "        prob_2.append(mix_expected_result)\n",
    "        \n",
    "        ## 2-3: calculate the inverse of delta**2 for students on each topic and for each item\n",
    "        # 2-3-a: calculate the inverse of delta**2 inversefor students on each topic\n",
    "        u_delta_square_inv = np.zeros(tSize)\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                u_delta_square_inv[m] = (\n",
    "                    u_topic_expected_result[m] * (1 - u_topic_expected_result[m]) *\n",
    "                    (Q ** 2) * (q_impact ** 2))##**(-1)\n",
    "        # 2-3-b: delta**2 inverse for item (don't forget to inverse it if it s not done in the future)\n",
    "        q_delta_square_inv = (\n",
    "                q_expected_result * (1 - q_expected_result) *\n",
    "                (Q ** 2) * (u_impact_avg ** 2))##**(-1)\n",
    "        ## 2-4: calculate the amount of difference between expected value of the game and actual value from student and item perspective\n",
    "        # 2-4-a: for student on each topic\n",
    "        u_topic_difference = np.zeros(tSize)\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                u_topic_difference[m] = q_impact * (correct - u_topic_expected_result[m])\n",
    "        # 2-4-b: for item\n",
    "        d_difference = u_impact_avg * (1 - correct - q_expected_result) # or, we can calculate the difference based on each topic separately and then get the average on the result. \n",
    "        ## 2-5: calculate the  denom coef for students on each topic and each item\n",
    "        # 2-5-a: for student on each topic\n",
    "        denom_u_topic = np.zeros(tSize)\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                denom_u_topic[m] = c_sigma[m]** float(-2) + u_delta_square_inv[m]\n",
    "        # 2-5-b: for each item\n",
    "        denom_q = d_sigma ** float(-2) + q_delta_square_inv\n",
    "        \n",
    "        ## 2-6: calculate the updated rating for student on each topic and item\n",
    "        # 2-6-a: for student on each topic   \n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                if correct == 1:\n",
    "                    change_correct = Q / denom_u_topic[m] * u_topic_difference[m]\n",
    "                    if abs(change_correct) <= 10:\n",
    "                        change_correct = 10\n",
    "                    learner_competency[uid, m] = learner_competency[uid, m] + change_correct\n",
    "                else:\n",
    "                    change_incorrect = 0.8*Q / denom_u_topic[m] * u_topic_difference[m]\n",
    "                    if abs(change_incorrect) <= 8:\n",
    "                        change_incorrect = -8\n",
    "                    learner_competency[uid, m] = learner_competency[uid, m] + change_incorrect\n",
    "                update = Q / denom_u_topic[m] * u_topic_difference[m]\n",
    "                learner_sigma[uid, m] = math.sqrt(1. / denom_u_topic[m])\n",
    "        # 2-6-b: for item\n",
    "        question_difficulty[qid] = question_difficulty[qid] + Q / denom_q * d_difference\n",
    "        question_sigma[qid] = math.sqrt(1. / denom_q) # check if it does not require -1\n",
    "    print(\"multivariate Glicko execution for learning item difficulty is ended.\")\n",
    "    return actual, prob_1, prob_2 # later, delete returning values from this function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_topic_glicko_students_rating_with_thresholds(df):\n",
    "    \"\"\"\n",
    "    Implementation of multi-variate_Glicko rating system for adaptive educational learning platforms.\n",
    "    This function is used to update the competency of students based on the difficulty of items obtained\n",
    "    from multi_topic_glicko_item_training_with_thresholds function. Please be advised that in this function, \n",
    "    the difficulty of items do not change and only the competency of students are changing. \n",
    "    \n",
    "    Arguments:\n",
    "    df -- train data in the form of Pandas data frame\n",
    "    \n",
    "    Output:\n",
    "    \"\"\"\n",
    "    print(\"multivariate Glicko execution for estimating student competency is started.\") \n",
    "    # For students we create a dataframe in which \n",
    "    correct = 0 #whether the question was not correctly or not\n",
    "    prob_1 = [] # a list for storing the probability of a correct response to a question\n",
    "    prob_2 = [] # a list fpr expected outcome by both RDs\n",
    "    actual = [] # a list for storing the actual response to a question.\n",
    "    response = np.zeros((len(df), 1))\n",
    "    d_square_inv = 0\n",
    "    difference = 0\n",
    "    for count, (index, item) in enumerate(df.iterrows()): \n",
    "        # Step 0: Initialization\n",
    "        uid = item['user_id']\n",
    "        qid = item['item_id']\n",
    "        timestamp = item['timestamp']\n",
    "        correct = item['correct']\n",
    "        actual.append(correct) # keeping the actual outcome of each interaction.\n",
    "        d = question_difficulty[qid] #getting the difficulty of the question qid from difficulty matrix\n",
    "        d_sigma = question_sigma[qid] #getting the sigma for difficulty of the question qid from difficulty matrix\n",
    "        c = learner_competency[uid] * q_mat[qid]\n",
    "        c_avg = np.sum(c)/np.count_nonzero(c)\n",
    "        c_sigma = learner_sigma[uid] * q_mat[qid]        \n",
    "        \n",
    "        # Step 1: Update the Rating deviation for the student as the result of passage of time.\n",
    "        # If it is the first time of attempting a topic, then, passage of time is zero and this function does nothing.\n",
    "        c_sigma = sigma_time_update(uid, qid, c_sigma, timestamp)\n",
    "        c_sigma_avg = np.sum(c_sigma)/np.count_nonzero(c_sigma)\n",
    "        \n",
    "        # Step 2: Calculate required coefficients\n",
    "        ## 2-1: Calculate g(RD) for items and students\n",
    "        # 2-1-a: g(RD) for item\n",
    "        q_impact = reduce_impact(d_sigma)\n",
    "        # 2-1-b: g(RD) for student on each topic\n",
    "        u_impact = np.zeros(tSize)\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                u_impact[m] = reduce_impact(c_sigma[m]) \n",
    "        u_impact_avg = np.sum(u_impact)/np.count_nonzero(u_impact) # or reduce_impact(c_sigma_avg) \n",
    "        ## 2-2: calculate the expected result from student and item point of view\n",
    "        # 2-2-a: calculate the expected outcome based on proficiency on each topic\n",
    "        u_topic_expected_result = np.zeros(tSize)\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "#                 print(m)\n",
    "#                 print(\"c[m] is\", c[m])\n",
    "                u_topic_expected_result[m] = expect_score(c[m], d, q_impact)   \n",
    "        # 2-2-b: calculate the expected outcome based on proficiency on all topics\n",
    "        u_item_expected_result = expect_score(c_avg, d, q_impact)\n",
    "        prob_1.append(u_item_expected_result)\n",
    "        # 2-2-c: calculate the expected outcome from the item point of view\n",
    "        q_expected_result = expect_score(d, c_avg, u_impact_avg)\n",
    "        # 2-2-d: calculate the expected outcome of a question for a student based on both rating deviations\n",
    "        mix_sigma = math.sqrt(c_sigma_avg**2 + d_sigma **2)\n",
    "        mix_impact = reduce_impact(mix_sigma)\n",
    "        mix_expected_result = expect_score(c_avg, d, mix_impact)\n",
    "        prob_2.append(mix_expected_result)\n",
    "        \n",
    "        ## 2-3: calculate the inverse of delta**2 for students on each topic and for each item\n",
    "        # 2-3-a: calculate the inverse of delta**2 inversefor students on each topic\n",
    "        u_delta_square_inv = np.zeros(tSize)\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                u_delta_square_inv[m] = (\n",
    "                    u_topic_expected_result[m] * (1 - u_topic_expected_result[m]) *\n",
    "                    (Q ** 2) * (q_impact ** 2))##**(-1)\n",
    "        ## 2-4: calculate the amount of difference between expected value of the game and actual value from student and item perspective\n",
    "        # 2-4-a: for student on each topic\n",
    "        u_topic_difference = np.zeros(tSize)\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                u_topic_difference[m] = q_impact * (correct - u_topic_expected_result[m])\n",
    "        ## 2-5: calculate the  denom coef for students on each topic and each item\n",
    "        # 2-5-a: for student on each topic\n",
    "        denom_u_topic = np.zeros(tSize)\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                denom_u_topic[m] = c_sigma[m]** float(-2) + u_delta_square_inv[m]\n",
    "        \n",
    "        ## 2-6: calculate the updated rating for student on each topic and item\n",
    "        # 2-6-a: for student on each topic\n",
    "        for m in range(tSize):\n",
    "            if q_mat[qid,m] != 0:\n",
    "                if correct == 1:\n",
    "                    change_correct = Q / denom_u_topic[m] * u_topic_difference[m]\n",
    "#                     print(change_correct)\n",
    "                    if abs(change_correct) <= 10:\n",
    "                        change_correct = 10\n",
    "                    learner_competency[uid, m] = learner_competency[uid, m] + change_correct\n",
    "                else:\n",
    "                    change_incorrect = 0.8*Q / denom_u_topic[m] * u_topic_difference[m]\n",
    "#                     print(change_incorrect)\n",
    "                    if abs(change_incorrect) <= 8:\n",
    "                        change_incorrect = -8\n",
    "                    learner_competency[uid, m] = learner_competency[uid, m] + change_incorrect\n",
    "                update = Q / denom_u_topic[m] * u_topic_difference[m]\n",
    "                learner_sigma[uid, m] = math.sqrt(1. / denom_u_topic[m])\n",
    "    print(\"multivariate Glicko execution for estinating student competency is ended.\")\n",
    "    return actual, prob_1, prob_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    'algebra05' : {\n",
    "    'folder' : 'data/kdd',\n",
    "    'course' : 'algebra05',\n",
    "    'min_interactions_per_user' : 5,\n",
    "    'kc_column' : 'KC(Default)', \n",
    "    'train_file' : 'algebra_2005_2006_train.txt', \n",
    "    'test_file' : 'algebra_2005_2006_master.txt'\n",
    "    },    \n",
    "    'algebra06' : {\n",
    "    'folder' : 'data/kdd',\n",
    "    'course' : 'algebra06',\n",
    "    'min_interactions_per_user' : 5,\n",
    "    'kc_column': 'KC(Default)',\n",
    "    'train_file' : 'algebra_2006_2007_train.txt',\n",
    "    'test_file' : 'algebra_2006_2007_master.txt'\n",
    "        \n",
    "    },\n",
    "    'bridge_algebra06' : {\n",
    "    'folder' : 'data/kdd',\n",
    "    'course' : 'bridge_algebra06',\n",
    "    'min_interactions_per_user' : 5,\n",
    "    'kc_column': 'KC(SubSkills)', \n",
    "    'train_file': 'bridge_to_algebra_2006_2007_train.txt', \n",
    "    'test_file': 'bridge_to_algebra_2006_2007_master.txt'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DETEMINE THE COURSE ID IN THE FOLLOWING CELL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dettermine course_id\n",
    "course_id = 'algebra05'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = configurations[course_id]['folder']\n",
    "course = configurations[course_id]['course']\n",
    "min_interactions_per_user = configurations[course_id]['min_interactions_per_user']\n",
    "kc_column = configurations[course_id]['kc_column']\n",
    "train_file = configurations[course_id]['train_file']\n",
    "test_file = configurations[course_id]['test_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing the row data made available by KDD organisers\n",
    "pre_processed_data, q_mat, listOfKC, dict_of_kc, train_set, test_set = kdd_pdr.prepare_kddcup10(folder, course, \\\n",
    "                                                                   train_file, \\\n",
    "                                                                   test_file,\\\n",
    "                                                                    kc_column, min_interactions_per_user,\\\n",
    "                                                                    True, False, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Reading Train and Test sets.\")\n",
    "# train_set = pd.read_csv(folder + '/'+course+\"/processed/train_set.csv\")\n",
    "# test_set = pd.read_csv(folder + '/'+course+\"/processed/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of train set is:\", train_set.shape)\n",
    "print(\"Shape of test set is:\", test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of students\n",
    "uSize = pre_processed_data['user_id'].nunique()\n",
    "print(\"Number of students is:\", uSize)\n",
    "\n",
    "qSize = pre_processed_data['item_id'].nunique()\n",
    "print(\"Number of questions is:\", qSize)\n",
    "\n",
    "tSize = len(listOfKC)\n",
    "print(\"Number of topics is:\", tSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training for learning item difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialization for learnin the difficulty of learning items####\n",
    "question_difficulty = np.full(qSize, 1500.00) #stores difficulty level of each question after each answer\n",
    "question_sigma = np.full(qSize, 350.00) \n",
    "learner_competency = np.full((uSize, tSize), 1500.00) #stores student's profiency level on each topic and is updated upon each attempt\n",
    "learner_sigma = np.full((uSize, tSize), 350.00) #stores student's uncertainty level on each topic and is updated upon each attempt\n",
    "question_counter = np.zeros(qSize) #number of time a question was answered by each user\n",
    "response_counter = np.zeros((uSize, tSize)) # number of times a question on the defined topic is solved\n",
    "\n",
    "'''\n",
    "For students, we create a dataframe, in which index column corresponds\n",
    "to student_id and columns correspond to the topic of the course. It is \n",
    "first populated by all zeros and then it is updated accordingly. \n",
    "If the response_counter is zero, it shows that it is the first time it has\n",
    "been attempted. Otherwise, the timestamp should be updated accordingly.\n",
    "'''\n",
    "attempted_at = pd.DataFrame(None, index=np.arange(uSize), columns=list(dict_of_kc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Calling Glicko function for train data to learn the difficulty of items ###\n",
    "## when doing training for learning item difficulty, don't forget to first sort values by their timestamp. \n",
    "train_set.sort_values(by=\"timestamp\", inplace=True) #first, timestamp should be converted to datetime\n",
    "train_set.reset_index(inplace=True, drop=True)\n",
    "actual, prob_1, prob_2 = multi_topic_glicko_item_training_with_thresholds(train_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training for learning user competency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calling Glicko function for train data to learn competency of students ###\n",
    "## Re-initializing students parameters\n",
    "learner_competency = np.full((uSize, tSize), 1500.00) \n",
    "learner_sigma = np.full((uSize, tSize), 350.00) \n",
    "question_counter = np.zeros(qSize) \n",
    "response_counter = np.zeros((uSize, tSize)) \n",
    "attempted_at = pd.DataFrame(None, index=np.arange(uSize), columns=list(dict_of_kc.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actual, prob_1, prob_2 = multi_topic_glicko_students_rating_with_thresholds(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test for learning item difficulty "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_competency_tmp = learner_competency.copy()\n",
    "learner_sigma_tmp = learner_sigma.copy()\n",
    "attempted_at_tmp = attempted_at.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Calling Glicko function for train data to learn the difficulty of items ###\n",
    "## when doing training for learning item difficulty, don't forget to first sort values by their timestamp. \n",
    "test_set.sort_values(by=\"timestamp\", inplace=True) #first, timestamp should be converted to datetime\n",
    "test_set.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, prob_1, prob_2 = multi_topic_glicko_item_training_with_thresholds(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test for learning user rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_competency = learner_competency_tmp.copy()\n",
    "learner_sigma = learner_sigma_tmp.copy()\n",
    "attempted_at = attempted_at_tmp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual, prob_1, prob_2 = multi_topic_glicko_students_rating_with_thresholds(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test = CalculateRMSE(prob_1, actual, len(prob_1))\n",
    "auc_test = auc_roc(actual, prob_1)\n",
    "acc_test = accuracy_score(np.array(actual), np.array(prob_1).round(), normalize=True)\n",
    "print(\"Test RMSE: \", rmse_test)\n",
    "print(\"Test AUC: \", auc_test)\n",
    "print(\"Test ACC: \", acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
